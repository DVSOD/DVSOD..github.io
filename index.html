<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Saliency Revisited using RGBD Videos: A Unified Dataset and Benchmark">
  <meta name="keywords" content="Saliency Detection">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DVSOD Benchmark</title>

	<meta property="og:title" content="Saliency Revisited using RGBD Videos: A Unified Dataset and Benchmark" />
	<meta property="og:description" content="DVSOD Benchmark" />


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VFNFH9CKNX"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-VFNFH9CKNX');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./resources/carousel-horse.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <!-- <script src="./static/js/index.js"></script> -->
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">

      <br>

      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3 publication-title" style="font-size: 2.2rem;"> Saliency Revisited using RGBD Videos: A Unified <br> Dataset and Benchmark</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
               <strong> Anonymous Author(s) </strong> <br>
               Under Review
            </span>
          </div>
          
          <span class="link-block">
            <a href="https://github.com/DVSOD/DVSOD-DViSal"
               class="external-link button is-normal is-rounded is-dark">
              <span>Dataset</span>
            </a>
          </span>

          <span class="link-block">
            <a href="https://github.com/DVSOD/DVSOD-Baseline"
               class="external-link button is-normal is-rounded is-dark">
              <span>Code</span>
            </a>
          </span>

          <span class="link-block">
            <a href="https://github.com/DVSOD/DVSOD-Evaluation"
               class="external-link button is-normal is-rounded is-dark">
              <span>Evaluation</span>
            </a>
          </span>

          
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero intro">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="intro" autoplay muted loop playsinline height="100%">
        <source src="resources/intro.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle">
        Illustration of the advantages of employing RGBD videos for detecting salient objects in a scene.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
              Salient object detection (SOD) aims to identify standout elements in a scene, with recent advancements primarily focused on integrating depth data (RGB-D) or temporal data from videos to enhance SOD in complex scenes. However, the unison of two types of crucial information remains largely underexplored due to data constraints. We address this gap by introducing the DViSal dataset, fueling further research in the emerging field of RGB-D video salient object detection (DVSOD). Our dataset features 237 diverse RGB-D videos alongside comprehensive annotations, including object and instance-level markings, as well as bounding boxes and scribbles. These resources enable a broad scope for potential research directions. We also conduct benchmarking experiments using various SOD models, affirming the efficacy of multimodal video input for salient object detection. Lastly, we highlight some intriguing findings and promising future research avenues. To foster growth in this field, our dataset and benchmark results are publicly accessible.
              <br>
              <strong> The website is currently undergoing construction. In the meantime, you can directly access our data and code by clicking the black button above.  </strong>
	        </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

 <!--/ 
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Video Demo</h2>

        <h2 class="title is-5">Visual Results</h2>
        <div class="content has-text-justified">
          <p>
              In the following video demo, we provide intuitive visual results for different scenarios, day and night, in the MVSeg dataset. We also highlighted the details with red boxes. Obviously, the results from our MVNet model are more complete compared to RGB-based DeeplabV3+. This attributes to the superiority of our method in engaging the advantages of complementary multispectral and temporal contexts.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="horse-real" autoplay controls muted preload loop playsinline style="max-width: 92%; display: block; margin: auto;">
            <source src="resources/results.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>

        <h2 class="title is-5">Method Quickview</h2>
        <div class="content has-text-justified">
          <p>
            The following animation presents an overview of the proposed MVNet. Starting from the input multispectral video, its pipeline consists of four parts: (a) feature extraction to obtain the multispectral video features; (b) an MVFuse module to furnish the query features with the rich semantic cues of memory frames; (c) an MVRegulator loss to regularize the multispectral video embedding space; and (d) a cascaded decoder to generate the final segmentation mask.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="horse-real" autoplay controls muted preload loop playsinline style="max-width: 95%; display: block; margin: auto;">
            <source src="resources/method.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br>

      </div>
    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Our BibTeX</h2>
    <pre><code>@InProceedings{ji2023mvss,
      title     = {Multispectral Video Semantic Segmentation: A Benchmark Dataset and Baseline},
      author    = {Ji, Wei and Li, Jingjing and Bian, Cheng and Zhou, Zongwei and Zhao, Jiaying and Yuille, Alan L. and Cheng, Li},
      booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
      month     = {June},
      year      = {2023},
      pages     = {1094-1104}
}</code></pre>
  </div>
</section>

<section class="section" id="Announcement">
  <div class="container is-max-desktop content">
    <h2 class="title">Announcement</h2>
    <p>
      Our code and dataset are currently undergoing routine internal approval processes due to patent and privacy considerations. They will be publicly available in mid-July. If you have any further questions, please email us at <font color="blue">wji3@ualberta.ca</font>.
      </p>
  </div>
</section>
-->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This webpage template is adapted from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0 License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
